#+TITLE: pandas
#+AUTHOR:    Karthik
#+OPTIONS: num:nil
# +OPTIONS: ^:nil p:t
# +OPTIONS: toc:nil  Timestamp: nil toc:nil date:nil author:nil
# +OPTIONS: broken-links:t
#+SETUPFILE: ~/mynotes/org2html/org-theme-collection/theme2.setup

* Resources

Youtube Corey Schafer Pandas Playlist :https://www.youtube.com/watch?v=ZyhVh-qRZPA&list=PL-osiE80TeTsWmV9i9c58mdDCSskIFdDS
10 min  pandas : https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html

* Install pandas 
install pandas
#+begin_src sh
pip install pandas
pip install jupyter notebook
pip install jupyterlab
#+end_src

download data
stack Overflow Annual Developer Survey
* Import and Export
file:./pandas-sub-doc/Import-Export.html
** Import 
#+begin_src python
import pandas as pd

df = pd.read_csv('data/survey_results_public.csv')
# Set index from the file 
df = pd.read_csv('data/survey_results_public.csv',index=dates)
df = pd.read_excel(open('tmp.xlsx', 'rb'),
              sheet_name='Sheet3')
# if index is date or datetime you need to convert it datetime formate to sort or do datetime operations
df = pd.read_clipboard(sep=r"[ ]{2,}")  #https://stackoverflow.com/questions/61605415/how-can-we-resample-ohlcv-1-minute-pandas-dataframe-into-a-5-minute-dataframe
df = pf.read_html('https://simple.wikiepedia.org/wiki/List_of_U.S._states')
# TODO read from databsae : sqllite3, mysql,psql, mongodb
#+end_src
For more information on read excel [[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html#][link]]
** Export

*** Export to csv
*** Export to excel 
#+begin_src python
import openpyxl
file_rpath = '../database/option_trading-banknifty/HDFCBANK_historical_data'# os.path.splittext('/temp/test.txt'))
file_name = file_rpath+".xlsx"
# df.to_excel(file_name,sheet_name='1minute',,index=False) can't write other sheets in this file
# if we use =to_excel= to write next sheet then it will rewrite entire file and all the data will be lost 

with pd.ExcelWriter(file_name, mode="w",engine="openpyxl")as writer:
    df.to_excel(writer, sheet_name="1minute")
    df_temp =down_sample_by_minute(df,2)
    df_temp.to_excel(writer,sheet_name='2minute')
    df_temp =down_sample_by_minute(df,3)
    df_temp.to_excel(writer,sheet_name='3minute')
    df_temp =down_sample_by_minute(df,5)
    df_temp.to_excel(writer,sheet_name='5minute')
#+end_src

*** Export to html
*** Export to markdown
#+begin_src python
import numpy as np
from sklearn.datasets import load_iris
import pandas as pd
 
data = load_iris()
df = pd.DataFrame(data.data,
                  columns=data.feature_names)
 
# Converts the dataframe into str object with formatting
print(df.to_markdown())
#+end_src

*** Export to database sqllite3
#+begin_src python
from sqlalchemy import create_engine
import psycopg2

engine = create_engine('postgresql://dbuser:dbpass@localhost:5432/sample_db')

# To READ or IMPORT 
sql_df = pd.read_sql('sample_table', engine, index_col='Respondent')

# TO EXPORT or WRITE
india_df.to_sql('sample_table', engine, if_exists='replace')
#+end_src


For sqlite

https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#reading-tables

https://stackoverflow.com/questions/36028759/how-to-open-and-convert-sqlite-database-to-pandas-dataframe


#+begin_src python
import sqlite3
import pandas as pd
# Create your connection.
cnx = sqlite3.connect('file.db')

df = pd.read_sql_query("SELECT * FROM table_name", cnx)
#+end_src

*** Export to org-mode or psql formate
#+begin_src python
from tabulate import tabulate
import pandas as pd

df = pd.DataFrame({'col_two' : [0.0001, 1e-005 , 1e-006, 1e-007],
                   'column_3' : ['ABCD', 'ABCD', 'long string', 'ABCD']})

print(tabulate(df, headers='keys', tablefmt='psql'))

# +----+-----------+-------------+

# |    |   col_two | column_3    |
# |----+-----------+-------------|
# |  0 |    0.0001 | ABCD        |
# |  1 |    1e-05  | ABCD        |
# |  2 |    1e-06  | long string |
# |  3 |    1e-07  | ABCD        |

# +----+-----------+-------------+

#+end_src



| 1   |   col two | column 3   |
|----+-----------+-------------|
|  0 |    0.0001 | ABCD        |
|  1 |    1e-05  | ABCD        |
|  2 |    1e-06  | long string |
|  3 |    1e-07  | ABCD        |
* View
#+begin_src python  
df #
display(df) # style  works only in jupyter not in terminal 
print(df) # plane text

df.tail(10)
df.head(10)
# To set the maximum rows and columns to view 
pd.set_option('display.max_columns', 85)
pd.set_option('display.max_rows', 85)

# to get the no.of rows and no.of columns 
df.shape  # (88883,85) # no.of rows:88883 , # no.of columns: 85
#+end_src
* Create dataframe
Types of creating dataframe
- using dictionary 
** Using dictionary each =key is column name=  =value is list_of elements in column= 
#+begin_src python 
people = {
    "first": ["Corey", 'Jane', 'John'], 
    "last": ["Schafer", 'Doe', 'Doe'], 
    "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com'],
    "employee id": ["2022B001","2022B002","2022B003"]
}

import pandas as pd

df = pd.DataFrame(people)
#+end_src

|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |
** Create Empty dataframe
#+begin_src python
data = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'volume'])
print(data)
#Empty DataFrame
#Columns: [date, open, high, low, close, volume]
#Index: []

# display(data)
#+end_src

** TODO Create db with list of dictionary
#+begin_src python
my_list = [{'date': datetime.datetime(2022, 8, 1, 9, 15, tzinfo=tzoffset(None, 19800)),
        'open': 529.05, 'high': 529.95, 'low': 528.9, 'close': 529.8, 'volume': 99353},
    {'date': datetime.datetime(2022, 8, 1, 9, 16, tzinfo=tzoffset(None, 19800)),
        'open': 529.8, 'high': 530.35, 'low': 529.5, 'close': 529.8, 'volume': 50971},
    {'date': datetime.datetime(2022, 8, 1, 9, 17, tzinfo=tzoffset(None, 19800)),
        'open': 529.75, 'high': 530, 'low': 528.65, 'close': 528.65, 'volume': 76733},
    {'date': datetime.datetime(2022, 8, 1, 9, 18, tzinfo=tzoffset(None, 19800)),
        'open': 528.75, 'high': 528.75, 'low': 528, 'close': 528.35, 'volume': 50487},
    {'date': datetime.datetime(2022, 8, 1, 9, 19, tzinfo=tzoffset(None, 19800)),
        'open': 528.35, 'high': 528.4, 'low': 527.65, 'close': 527.9, 'volume': 45933},
    {'date': datetime.datetime(2022, 8, 1, 9, 20, tzinfo=tzoffset(None, 19800)),
        'open': 527.9, 'high': 527.9, 'low': 526.5, 'close': 526.5, 'volume': 61776},
    {'date': datetime.datetime(2022, 8, 1, 9, 21, tzinfo=tzoffset(None, 19800)),
        'open': 526.65, 'high': 527.05, 'low': 526.4, 'close': 526.9, 'volume': 52064},
    {'date': datetime.datetime(2022, 8, 1, 9, 22, tzinfo=tzoffset(None, 19800)),
        'open': 527, 'high': 527.8, 'low': 526.4, 'close': 527.8, 'volume': 44571},
    {'date': datetime.datetime(2022, 8, 1, 9, 23, tzinfo=tzoffset(None, 19800)),
        'open': 527.55, 'high': 527.9, 'low': 527.3, 'close': 527.65, 'volume': 29982},
    {'date': datetime.datetime(2022, 8, 1, 9, 24, tzinfo=tzoffset(None, 19800)),
        'open': 527.5, 'high': 528.4, 'low': 527.45, 'close': 528.25, 'volume': 51639},
    {'date': datetime.datetime(2022, 8, 1, 9, 25, tzinfo=tzoffset(None, 19800)), 
        'open': 528.3, 'high': 529.4, 'low': 528.3, 'close': 529.2, 'volume': 70639}, 
        {'date': datetime.datetime(2022, 8, 1, 9, 26, tzinfo=tzoffset(None, 19800)), 
        'open': 529.2, 'high': 529.2, 'low': 528.3, 'close': 528.55, 'volume': 46564}]

data = pd.DataFrame(my_list)
#+end_src

|    | date                |   open |   high |    low |   close |   volume |
|----+---------------------+--------+--------+--------+---------+----------|
|  0 | 2022-08-01 09:15:00 | 529.05 | 529.95 | 528.9  |  529.8  |    99353 |
|  1 | 2022-08-01 09:16:00 | 529.8  | 530.35 | 529.5  |  529.8  |    50971 |
|  2 | 2022-08-01 09:17:00 | 529.75 | 530    | 528.65 |  528.65 |    76733 |
|  3 | 2022-08-01 09:18:00 | 528.75 | 528.75 | 528    |  528.35 |    50487 |
|  4 | 2022-08-01 09:19:00 | 528.35 | 528.4  | 527.65 |  527.9  |    45933 |
|  5 | 2022-08-01 09:20:00 | 527.9  | 527.9  | 526.5  |  526.5  |    61776 |
|  6 | 2022-08-01 09:21:00 | 526.65 | 527.05 | 526.4  |  526.9  |    52064 |
|  7 | 2022-08-01 09:22:00 | 527    | 527.8  | 526.4  |  527.8  |    44571 |
|  8 | 2022-08-01 09:23:00 | 527.55 | 527.9  | 527.3  |  527.65 |    29982 |
|  9 | 2022-08-01 09:24:00 | 527.5  | 528.4  | 527.45 |  528.25 |    51639 |
| 10 | 2022-08-01 09:25:00 | 528.3  | 529.4  | 528.3  |  529.2  |    70639 |
| 11 | 2022-08-01 09:26:00 | 529.2  | 529.2  | 528.3  |  528.55 |    46564 |

** Create with json data or dictionary
** Create empty column
#+begin_src
data = yf.download("MSFT", period ='2d', interval="1m")    # Working from april 2020
del data['Close']
data.rename(columns={'Open': 'open','High':'high','Low':'low','Adj Close' : 'close','Volume':'volume'}, inplace=True)
data = data.tz_localize(None)

data=data.assign(gain="",loss='',rsi='',ma_fast='',ma_slow='',macd='')
print(data)
#+end_src
** Add Empty Colum to existing dataframe
#+begin_src python
df = pd.DataFrame({"A": [1,2,3], "B": [2,3,4]})
df
#Out[18]:
#   A  B
#0  1  2
#1  2  3
#2  3  4

df.assign(C="",D=np.nan)
#Out[21]:
#   A  B C   D
#0  1  2   NaN
#1  2  3   NaN
#2  3  4   NaN
#+end_src
* Header or Column Name or Field 
** Get all Header or Field or Columns Name
#+begin_src python
# Import pandas package
import pandas as pd
	
# making data frame
data = pd.read_csv("nba.csv")

# iterating the columns
for col in data.columns:
	print(col)
# Method 2 
print(list(data.columns)

# Method 3
print(data.keys())

# Method 4
print(list(data.columns.values))

# Method 5
print(list(data.columns.values.tolist()))
#+end_src

** rename  header or column name
#+begin_src python
df[['ConvertedComp','Hobbyist','Country']]
#+end_src

|   Respondent |    ConvertedComp | Hobbyist   | Country                                   |
|--------------+------------------+------------+-------------------------------------------|
|            1 |    nan           | Yes        | United Kingdom                            |
|            2 |    nan           | No         | Bosnia and Herzegovina                    |
|            3 |   8820           | Yes        | Thailand                                  |
|            4 |  61000           | No         | United States                             |
|            5 |    nan           | Yes        | Ukraine                                   |

#+begin_src python
df.rename(columns={'ConvertedComp': 'SalaryUSD'}, inplace=True)
df[['SalaryUSD','Hobbyist','Country']]
#+end_src

|   Respondent |    ConvertedComp | Hobbyist   | Country                                   |
|--------------+------------------+------------+-------------------------------------------|
|            1 |    nan           | Yes        | United Kingdom                            |
|            2 |    nan           | No         | Bosnia and Herzegovina                    |
|            3 |   8820           | Yes        | Thailand                                  |
|            4 |  61000           | No         | United States                             |
|            5 |    nan           | Yes        | Ukraine                                   |
** Add Empty Colum to existing dataframe
#+begin_src python
df = pd.DataFrame({"A": [1,2,3], "B": [2,3,4]})
df
#Out[18]:
#   A  B
#0  1  2
#1  2  3
#2  3  4

df.assign(C="",D=np.nan)
#Out[21]:
#   A  B C   D
#0  1  2   NaN
#1  2  3   NaN
#2  3  4   NaN
#+end_src
** Drop or delete Column
#+begin_src python
df = df.drop('column_name', axis=1)
df = df.drop(columns=['column_nameA', 'column_nameB'])

df.drop('column_name', axis=1, inplace=True)

df.drop(['column_nameA', 'column_nameB'], axis=1, inplace=True)

del df['column_name']
#+end_src
** Drop Row
#+begin_src python 
data.drop(["row_name1", "row_name2", "row_name3","row_name4"], inplace = True)
df = df.drop(df.index [ [ 0,2 ] ])

#+end_src
* getting data form dataframe

Get the specific column for dataframe 
#+begin_src python
df['email']
#+end_src

#+begin_src 
0    CoreyMSchafer@gmail.com
1          JaneDoe@email.com
2          JohnDoe@email.com
Name: email, dtype: object

#+end_src
or you can use as class elements to get the column
#+begin_src python
df.email
#+end_src

#+begin_src 
0    CoreyMSchafer@gmail.com
1          JaneDoe@email.com
2          JohnDoe@email.com
Name: email, dtype: object

#+end_src

Slice Column for dataframe 
#+begin_src python
df[['last', 'email']]
#+end_src

|   | last    | email                   |
|---+---------+-------------------------|
| 0 | Schafer | CoreyMSchafer@gmail.com |
| 1 | Doe     | JaneDoe@email.com       |
| 2 | Doe     | JohnDoe@email.com       |

Slice row form dataframe
#+begin_src python 
df[1:3]
#+end_src
|    | first   | last   | email             | employee id   |
|----+---------+--------+-------------------+---------------|
|  1 | Jane    | Doe    | JaneDoe@email.com | 2022B002      |
|  2 | John    | Doe    | JohnDoe@email.com | 2022B003      |

#+begin_src python
df.columns
#+end_src
#+begin_src 
Index(['first', 'last', 'email', 'employee id'], dtype='object')
#+end_src

To get the header list of dataframe
#+begin_src python 
list(df.columns.values)
#+end_src
** If index is datetime.date we can slice
=df['2022-01-01' :'2022-02-01']= we give you sliced dataframe from horizontal

You can get slice from loc and iloc also
* Set Index 
The index of pandas can be either a integer or float or string
we can set the index of the data frame like
#+begin_src python 
df.set_index("employee id")
#+end_src


|             | first | last    | email                   |
| employee id |       |         |                         |
|-------------+-------+---------+-------------------------|
| 2022B001    | Corey | Schafer | CoreyMSchafer@gmail.com |
| 2022B002    | Jane  | Doe     | JaneDoe@email.com       |
| 2022B003    | John  | Doe     | JohnDoe@email.com       |

We can reset the index by
#+begin_src python
df.reset_index(inplace=True)
#+end_src

|   | first | last    | email                   | employee id |
|---+-------+---------+-------------------------+-------------|
| 0 | Corey | Schafer | CoreyMSchafer@gmail.com | 2022B001    |
| 1 | Jane  | Doe     | JaneDoe@email.com       | 2022B002    |
| 2 | John  | Doe     | JohnDoe@email.com       | 2022B003    |
* unique element in row/column
To get the unique value in all column

#+begin_src python
people = {
    "first": ["Corey", 'Jane', 'John'], 
    "last": ["Schafer", 'Doe', 'Doe'], 
    "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com'],
    "employee id": ["2022B001","2022B002","2022B003"]
}

import pandas as pd
df = pd.DataFrame(people)
#+end_src


#+begin_src python
for col in df:
  print(df[col].unique())
#+end_src

#+begin_src 
['Corey' 'Jane' 'John']
['Schafer' 'Doe']
['CoreyMSchafer@gmail.com' 'JaneDoe@email.com' 'JohnDoe@email.com']
#+end_src


* Inplace
In general any operation performed in dataframe will not result in dataframe we need to assigne the change to the dataframe or

Pandas has inbuild function to save the change performed on dataframe
eg:
#+begin_src python
df.set_index("employee id") # no change in df
df = df.set_index("employee id") # df is updated
#or
df.set_index("employee id", inplace=True) # df is updated

#+end_src
* For loop in pandas
loop thorugh column
#+begin_src python
for (columnName, columnData) in inf_df.iteritems():
   print('Colunm Name : ', columnName)
   print('Column Contents : ', columnData.values)
#+end_src

column
#+begin_src python
for col in df:                                                                      
    alleles = list()                                                                
    for num, allele in enumerate(letterOrder):                                      
        alleles.append(df[col].str.count(allele).sum())                             

    full_results[col] = [letterOrder[np.argmax(alleles)], np.max(alleles)]
#+end_src

loop thouugh rows
#+begin_src python
for idx, row in df.iterrows():
    print(idx, row['Year'], row['Sales'])
# Returns:
# 0 2018 1000
# 1 2019 2300
# 2 2020 1900
# 3 2021 3400
#+end_src
* Selection
 data access methods, =.at=, =.iat=, =.loc= and =.iloc=.
** Lables (.loc)
We can access the row using =index value(loc)= or =index no(iloc)= \\
By default the index is =not set= then the =index value= will be row number starting with zero

If nothing else is specified, the values are labeled with their index number. First value has index 0, second value has index 1 etc.

This label can be used to access a specified value.

#+begin_src python
df.set_index("employee id",inplace=True)

df.loc['2022B003'] # Note need to set index
#+end_src

#+begin_src 
first                 John
last                   Doe
email    JohnDoe@email.com
Name: 2022B003, dtype: object
#+end_src

#+begin_src python
df.loc['2022B002':'2022B003'] # Note need to set index
#+end_src

| employee id   | first   | last   | email             |
|---------------+---------+--------+-------------------|
| 2022B002      | Jane    | Doe    | JaneDoe@email.com |
| 2022B003      | John    | Doe    | JohnDoe@email.com |

Slice by row and column
#+begin_src python
df.loc['2022B002':'2022B003',['first','email']] # Note need to set index
#+end_src

| employee id   | first   | email             |
|---------------+---------+-------------------|
| 2022B002      | Jane    | JaneDoe@email.com |
| 2022B003      | John    | JohnDoe@email.com |

#+begin_src python
people = {
    "first": ["Corey", 'Jane', 'John'], 
    "last": ["Schafer", 'Doe', 'Doe'], 
    "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com'],
    "employee id": ["2022B001","2022B002","2022B003"]
}
df = pd.DataFrame(people)
print(f'df.index.name :{df.index.name}')
print(f'df.index.values:{df.index.values}') 

df.set_index("employee id",inplace=True)
print('after setting index as employee')
print(f'df.index.name :{df.index.name}')
print(f'df.index.values:{df.index.values}') 
display(df)


df.index[df["first"]=='Jane'].tolist()
#+end_src

result 
#+begin_quote
df.index.name :None \\
df.index.values:[0 1 2] \\
after setting index as employee \\
df.index.name :employee id \\
df.index.values:['2022B001' '2022B002' '2022B003']
#+end_quote
** iloc
iloc is simillar to loc insted of index value we will use index number
#+begin_src python
df.set_index("employee id",inplace=True)
df.iloc[2] # Note need to set index
#+end_src

#+begin_src 
first                 John
last                   Doe
email    JohnDoe@email.com
Name: 2022B003, dtype: object
#+end_src

#+begin_src python
df.iloc[1:3] # Note need to set index
#+end_src


|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |

Slice by row and column in iloc 
#+begin_src python
df.iloc[[1,2],[0,2]]
#+end_src
|    | first   | email             |
|----+---------+-------------------|
|  1 | Jane    | JaneDoe@email.com |
|  2 | John    | JohnDoe@email.com |
** set value using iloc
#+begin_src python
idx = start_idx # 388
# for idx in range(data.shape[0]):
diff = data.iloc[idx]['close'] - data.iloc[idx-1]['close']
if diff >=0:
    data.iloc[idx,data.columns.get_loc("gain")] = diff
    data.iloc[idx,data.columns.get_loc('loss')] = 0

else:
    data.iloc[idx,data.columns.get_loc("gain")] = 0
    data.iloc[idx,data.columns.get_loc('loss')] = -1*diff
#+end_src
** at
** iat
** loc to  iloc and iloc to loc

#+begin_src python 
df.iloc[0].name # give loc Timestamp('2020-01-01 09:15:00')
named_index = df.iloc[0].name
df.index.get_loc(named_index)  # gives iloc value 
#+end_src

#+begin_src 
df.index.get_loc(window_stop_row.name)
#+end_src

#+begin_src python
#In [131]:
dates = pd.date_range('1/1/2000', periods=8)
df = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])
df

#Out[131]:
#                   A         B         C         D
#2000-01-01  0.095234 -1.000863  0.899732 -1.742152
#2000-01-02 -0.517544 -1.274137  1.734024 -1.369487
#2000-01-03  0.134112  1.964386 -0.120282  0.573676
#2000-01-04 -0.737499 -0.581444  0.528500 -0.737697
#2000-01-05 -1.777800  0.795093  0.120681  0.524045
#2000-01-06 -0.048432 -0.751365 -0.760417 -0.181658
#2000-01-07 -0.570800  0.248608 -1.428998 -0.662014
#2000-01-08 -0.147326  0.717392  3.138620  1.208639

#In [133]:    
window_stop_row = df[df.index < '2000-01-04'].iloc[-1]
window_stop_row.name

#Out[133]:
#Timestamp('2000-01-03 00:00:00', offset='D')

#In [134]:
df.index.get_loc(window_stop_row.name)

#Out[134]:
#2  # iloc value 
#+end_src
* Filters
https://datatofish.com/filter-pandas-dataframe/
https://www.statology.org/pandas-filter-by-index-value/

#+begin_src python
# '|': or comparisoin operator 
# '&' : and  comparison operator 
# '~' : not operator

filt = (df['last'] == 'Schafer') | (df['first'] == 'John')
filt
# Result 
# 0     True
# 1    False
# 2     True
# dtype: bool
df[filt]
#+end_src
|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |

#+begin_src python
df[~filt] # select which is not filtered
#+end_src
|    | first   | last   | email             | employee id   |
|----+---------+--------+-------------------+---------------|
|  1 | Jane    | Doe    | JaneDoe@email.com | 2022B002      |


Filter column 'LanguageWorkedWith' where it shouldn't have any =NaN= value
#+begin_src python
print(df['LanguageWorkedWith'])
#+end_src

#+begin_src 
Respondent
1                          HTML/CSS;Java;JavaScript;Python
2                                      C++;HTML/CSS;Python
3                                                 HTML/CSS
4                                      C;C++;C#;Python;SQL
5              C++;HTML/CSS;Java;JavaScript;Python;SQL;VBA
                               ...                        
88377                        HTML/CSS;JavaScript;Other(s):
88601                                                  NaN
88802                                                  NaN
88816                                                  NaN
88863    Bash/Shell/PowerShell;HTML/CSS;Java;JavaScript...
Name: LanguageWorkedWith, Length: 88883, dtype: object
#+end_src

#+begin_src python
filt
#+end_src

#+begin_src 
Respondent
1         True
2         True
3        False
4         True
5         True
         ...  
88377    False
88601    False
88802    False
88816    False
88863    False
Name: LanguageWorkedWith, Length: 88883, dtype: bool
#+end_src

#+begin_src python
df.loc[filt]
#+end_src
#+begin_src 
Respondent                                                   ...                                                   
1           I am a student who is learning to code      Yes  ...  Appropriate in length  Neither easy nor difficult
2           I am a student who is learning to code       No  ...  Appropriate in length  Neither easy nor difficult
4                   I am a developer by profession       No  ...  Appropriate in length                        Easy
5                   I am a developer by profession      Yes  ...  Appropriate in length                        Easy
8                      I code primarily as a hobby      Yes  ...  Appropriate in length  Neither easy nor difficult
...                                            ...      ...  ...                    ...                         ...
84539                                          NaN      Yes  ...  Appropriate in length                        Easy
85738                                          NaN      Yes  ...              Too short                        Easy
86566                                          NaN      Yes  ...  Appropriate in length                        Easy
87739                                          NaN      Yes  ...  Appropriate in length                        Easy
88212                                          NaN       No  ...  Appropriate in length                        Easy

[36443 rows x 84 columns]

#+end_src

#+begin_src python
df.loc[filt, 'LanguageWorkedWith']
#+end_src

#+begin_src 
Respondent
1                          HTML/CSS;Java;JavaScript;Python
2                                      C++;HTML/CSS;Python
4                                      C;C++;C#;Python;SQL
5              C++;HTML/CSS;Java;JavaScript;Python;SQL;VBA
8        Bash/Shell/PowerShell;C;C++;HTML/CSS;Java;Java...
                               ...                        
84539    Bash/Shell/PowerShell;C;C++;HTML/CSS;Java;Java...
85738      Bash/Shell/PowerShell;C++;Python;Ruby;Other(s):
86566      Bash/Shell/PowerShell;HTML/CSS;Python;Other(s):
87739             C;C++;HTML/CSS;JavaScript;PHP;Python;SQL
88212                           HTML/CSS;JavaScript;Python
Name: LanguageWorkedWith, Length: 36443, dtype: object
#+end_src
** filter get named index 
#+begin_src 
filter1 = (edf['breakout']==True)
# edf[(edf['breakout']==True)]
# display(filter1)
print(filter1.index)
print(filter1.index.values)
#+end_src

result 
#+begin_src 
DatetimeIndex(['2022-08-01 09:15:00', '2022-08-01 09:17:00',
               '2022-08-01 09:19:00', '2022-08-01 09:21:00',
               '2022-08-01 09:23:00', '2022-08-01 09:25:00',
               '2022-08-01 09:27:00', '2022-08-01 09:29:00',
               '2022-08-01 09:31:00', '2022-08-01 09:33:00',
               ...
               '2022-08-30 15:11:00', '2022-08-30 15:13:00',
               '2022-08-30 15:15:00', '2022-08-30 15:17:00',
               '2022-08-30 15:19:00', '2022-08-30 15:21:00',
               '2022-08-30 15:23:00', '2022-08-30 15:25:00',
               '2022-08-30 15:27:00', '2022-09-01 09:15:00'],
              dtype='datetime64[ns]', name='date', length=3741, freq=None)

['2022-08-01T09:15:00.000000000' '2022-08-01T09:17:00.000000000'
 '2022-08-01T09:19:00.000000000' ... '2022-08-30T15:25:00.000000000'
 '2022-08-30T15:27:00.000000000' '2022-09-01T09:15:00.000000000']

#+end_src

* Update element in dataframe
** Update element in data fram
#+begin_src python
people = {
     "first": ["Corey", 'Jane', 'John'], 
     "last": ["Schafer", 'Doe', 'Doe'], 
     "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com'],
     "employee id": ["2022B001","2022B002","2022B003"]
}

df = pd.DataFrame(people)
#+end_src



 |    | first   | last    | email                   | employee id   |
 |----+---------+---------+-------------------------+---------------|
 |  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
 |  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
 |  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |

*** Using loc
#+begin_src python 
df.loc[2] = ['John', 'Smith', 'JohnSmith@email.com']
#+end_src
 |   | first | last    | email                   | employee id |
 |---+-------+---------+-------------------------+-------------|
 | 0 | Corey | Schafer | CoreyMSchafer@gmail.com | 2022B001    |
 | 1 | Jane  | Doe     | JaneDoe@email.com       | 2022B002    |
 | 2 | John  | Smith   | JohnSmith@email.com     | 2022B003    |
#+begin_src python
df[filt]['last'] = 'Smith' #Error: Can't copy SettingWithCopyWarning
df.loc[2, 'last'] = 'Smith'
#+end_src
*** Using at
simillar to loc 
#+begin_src python
df.loc[2, 'last'] = 'Smith'
df.at[2, 'last'] = 'Doe'
#+end_src
*** Using fuctions 
#+begin_src python 
df['email'].str.upper()
#+end_src

#+begin_src
0    COREYMSCHAFER@GMAIL.COM
1          JANEDOE@EMAIL.COM
2          JOHNDOE@EMAIL.COM
Name: email, dtype: object
#+end_src
#+begin_src python 
df['email'].apply(len)
#+end_src
#+begin_src 
0    23
1    17
2    17
Name: email, dtype: int64
#+end_src


#+begin_src python
def update_email(email):
    return email.capitalize()
df['email'].apply(update_email)
#+end_src

#+begin_src 
0    Coreymschafer@gmail.com
1          Janedoe@email.com
2          Johndoe@email.com
Name: email, dtype: object
#+end_src

*** Using lambda
#+begin_src python
df['email'].apply(lambda x: x.upper())
#+end_src

#+begin_src 
0    coreymschafer@gmail.com
1          janedoe@email.com
2          johndoe@email.com
Name: email, dtype: object
#+end_src
*** Using list compression
#+begin_src python
df.columns = [x.upper() for x in df.columns]
#+end_src

 |   | FIRST | LAST    | EMAIL                   | EMPLOYEE ID |
 |---+-------+---------+-------------------------+-------------|
 | 0 | Corey | Schafer | CoreyMSchafer@gmail.com | 2022B001    |
 | 1 | Jane  | Doe     | JaneDoe@email.com       | 2022B002    |
 | 2 | John  | Smith   | JohnSmith@email.com     | 2022B003    |

*** Using pandas 'apply'
#+begin_src python 
df['email'].apply(len)
#+end_src

#+begin_src 
0    23
1    17
2    17
Name: email, dtype: int64
#+end_src

#+begin_src python
df.apply(len, axis='columns')
#+end_src
#+begin_src 
0    3
1    3
2    3
dtype: int64
#+end_src

#+begin_src python
df.apply(pd.Series.min)
#first                      Corey
#last                         Doe
#email    coreymschafer@gmail.com
#dtype: object
#+end_src

#+begin_src python
df.apply(lambda x: x.min())
#first                      Corey
#last                         Doe
#email    coreymschafer@gmail.com
#dtype: object
#+end_src
*** Using pandas 'applymap''
#+begin_src python
df.applymap(len)
# 	first 	last 	email
#0 	5 	7 	23
#1 	4 	3 	17
#2 	4 	5 	17
df.applymap(str.lower)
# 	first 	last 	email
#0 	corey 	schafer 	coreymschafer@gmail.com
#1 	jane 	doe 	janedoe@email.com
#2 	john 	smith 	johndoe@email.com
#+end_src
*** Using pandas 'map'
#+begin_src python
df['Hobbyist']
#+end_src

#+begin_src 
Respondent
1        Yes
2         No
3        Yes
4         No
5        Yes
        ... 
88377    Yes
88601     No
88802     No
88816     No
88863    Yes
Name: Hobbyist, Length: 88883, dtype: object
#+end_src

#+begin_src python
df['Hobbyist'].map({'Yes': True, 'No': False})
df['Hobbyist'] = df['Hobbyist'].map({'Yes': True, 'No': False})
df['Hobbyist']
#+end_src

#+begin_src 
Respondent
1         True
2        False
3         True
4        False
5         True
         ...  
88377     True
88601    False
88802    False
88816    False
88863     True
Name: Hobbyist, Length: 88883, dtype: bool
#+end_src
*** Error in map if some elements in column is not specific
#+begin_src python
df['first'].map({'Corey': 'Chris', 'Jane': 'Mary'})
#0    Chris
#1     Mary
#2      NaN
#Name: first, dtype: object
df['first'].replace({'Corey': 'Chris', 'Jane': 'Mary'})
#0    Chris
#1     Mary
#2     Smit
#Name: first, dtype: object
#+end_src
* Add and remove rows and columns
#+begin_src python 
people = {
    "first": ["Corey", 'Jane', 'John'], 
    "last": ["Schafer", 'Doe', 'Doe'], 
    "email": ["CoreyMSchafer@gmail.com", 'JaneDoe@email.com', 'JohnDoe@email.com'],
    "employee id": ["2022B001","2022B002","2022B003"]
}

import pandas as pd

df = pd.DataFrame(people)
#+end_src

|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |
** Add column 
#+begin_src python 
df['full name'] = df['first'] + ' ' + df['last']
df
#+end_src
|    | first   | last    | email                   | employee id   | full name    |
|----+---------+---------+-------------------------+---------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      | Corey Schafer |
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      | Jane Doe      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      | John Doe      |
** Remove column
#+begin_src python 
df.drop(columns=[ 'last','full name'], inplace=True)
#+end_src

|    | first   | email                   | employee id   |
|----+---------+-------------------------+---------------|
|  0 | Corey   | CoreyMSchafer@gmail.com | 2022B001      |
|  1 | Jane    | JaneDoe@email.com       | 2022B002      |
|  2 | John    | JohnDoe@email.com       | 2022B003      |
#+begin_src python 
df.drop(columns=['first', 'last'], inplace=True)
#+end_src

|    | email                   | employee id   | full name     |
|----+-------------------------+---------------+---------------|
|  0 | CoreyMSchafer@gmail.com | 2022B001      | Corey Schafer |
|  1 | JaneDoe@email.com       | 2022B002      | Jane Doe      |
|  2 | JohnDoe@email.com       | 2022B003      | John Doe      |
** Creating column
#+begin_src python
 df[['first', 'last']] = df['full_name'].str.split(' ', expand=True)
#+end_src

|    | email                   | employee id   | full name     | first   | last    |
|----+-------------------------+---------------+---------------+---------+---------|
|  0 | CoreyMSchafer@gmail.com | 2022B001      | Corey Schafer | Corey   | Schafer |
|  1 | JaneDoe@email.com       | 2022B002      | Jane Doe      | Jane    | Doe     |
|  2 | JohnDoe@email.com       | 2022B003      | John Doe      | John    | Doe     |

** Add new row in dataframe using append 
#+begin_src python 
df.append({'first': 'Tony'}, ignore_index=True) # NOTE this method is deprecated in future version use pandas.concat 
#+end_src
** Add two dataframe in row using append
#+begin_src python 
people = {
    'first': ['Tony', 'Steve'], 
    'last': ['Stark', 'Rogers'], 
    'email': ['IronMan@avenge.com', 'Cap@avenge.com']
}
df2 = pd.DataFrame(people)
#+end_src
|    | email              | full name    | first   | last   |
|----+--------------------+--------------+---------+--------|
|  0 | IronMan@avenge.com | Tony Stark   | Tony    | Stark  |
|  1 | Cap@avenge.com     | Steve Rogers | Steve   | Rogers |

#+begin_src python 
df.append(df2, ignore_index=True, sort=False)
#+end_src

|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |
|  3 | Tony    | Stark   | IronMan@avenge.com      | nan           |
|  4 | Steve   | Rogers  | Cap@avenge.com          | nan           |
** Remove row with index no or by value 
#+begin_src python 
df.drop(index=4)
#+end_src
|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  1 | Jane    | Doe     | JaneDoe@email.com       | 2022B002      |
|  2 | John    | Doe     | JohnDoe@email.com       | 2022B003      |
|  3 | Tony    | Stark   | IronMan@avenge.com      | nan           |
#+begin_src python 
filt = df['last'] == 'Doe'
df =df.drop(index=df[filt].index)
#+end_src

|    | first   | last    | email                   | employee id   |
|----+---------+---------+-------------------------+---------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com | 2022B001      |
|  3 | Tony    | Stark   | IronMan@avenge.com      | nan           |
|  4 | Steve   | Rogers  | Cap@avenge.com          | nan           |

* Sorting-Data

#+begin_src python 
people = {
    'first': ['Corey', 'Jane', 'John', 'Adam'], 
    'last': ['Schafer', 'Doe', 'Doe', 'Doe'], 
    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', 'A@email.com']
}
import pandas as pd
df = pd.DataFrame(people)
#+end_src

|    | first   | last    | email                   |
|----+---------+---------+-------------------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com |
|  1 | Jane    | Doe     | JaneDoe@email.com       |
|  2 | John    | Doe     | JohnDoe@email.com       |
|  3 | Adam    | Doe     | A@email.com             |
** Sort by one column 
#+begin_src python 
df.sort_values(by='last', ascending=True)
#+end_src

|    | first   | last    | email                   |
|----+---------+---------+-------------------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com |
|  1 | Jane    | Doe     | JaneDoe@email.com       |
|  2 | John    | Doe     | JohnDoe@email.com       |
|  3 | Adam    | Doe     | A@email.com             |
** Sort by two column 
#+begin_src python 
df.sort_values(by=['last', 'first'], ascending=False)
#+end_src
|    | first   | last    | email                   |
|----+---------+---------+-------------------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com |
|  2 | John    | Doe     | JohnDoe@email.com       |
|  1 | Jane    | Doe     | JaneDoe@email.com       |
|  3 | Adam    | Doe     | A@email.com             |
** Sort by two column with multiple sort values 
#+begin_src python 
df.sort_values(by=['last', 'first'], ascending=[False, True], inplace=True)
#+end_src
|    | first   | last    | email                   |
|----+---------+---------+-------------------------|
|  0 | Corey   | Schafer | CoreyMSchafer@gmail.com |
|  3 | Adam    | Doe     | A@email.com             |
|  1 | Jane    | Doe     | JaneDoe@email.com       |
|  2 | John    | Doe     | JohnDoe@email.com       |
+----+---------+---------+-------------------------+

#+begin_src python
df['last'].sort_values()
# 3        Doe
# 1        Doe
# 2        Doe
# 0    Schafer
# Name: last, dtype: object
#+end_src
** Example 2
#+begin_src python 
dir_name= '/media/jayradhey/myVolume/WorkSpace_Python/pandas/'
file_name='data/survey_results_public.csv'
schema_name='data/survey_results_schema.csv'
schema_path=dir_name + schema_name
file_path =  dir_name + file_name
print(file_path, schema_path)

import pandas as pd
df = pd.read_csv(file_path, index_col='Respondent') # , index_col='Respondent'
schema_df = pd.read_csv(schema_path, index_col='Column')
#+end_src

*** Sort Salary in Desending Order  based Country(in ascending Order)
#+begin_src python
df.sort_values(by=['Country', 'ConvertedComp'], ascending=[True, False], inplace=True)
df[['Country', 'ConvertedComp']].head(50) 
#+end_src

|   Respondent | Country     |   ConvertedComp |
|--------------+-------------+-----------------|
|        63129 | Afghanistan |           1e+06 |
|        50499 | Afghanistan |      153216     |
|        39258 | Afghanistan |       19152     |
|        58450 | Afghanistan |       17556     |
|         7085 | Afghanistan |       14364     |
|        22450 | Afghanistan |        7980     |
|        48436 | Afghanistan |        4464     |
|        10746 | Afghanistan |        3996     |
|         8149 | Afghanistan |        1596     |
|        29736 | Afghanistan |        1116     |
|          722 | Afghanistan |           0     |
|        28638 | Afghanistan |           0     |
|         6417 | Afghanistan |         nan     |
|         7353 | Afghanistan |         nan     |
|        12310 | Afghanistan |         nan     |
|        26340 | Afghanistan |         nan     |
|        29045 | Afghanistan |         nan     |
|        33178 | Afghanistan |         nan     |
|        37802 | Afghanistan |         nan     |
|        40000 | Afghanistan |         nan     |
|        43106 | Afghanistan |         nan     |
|        44403 | Afghanistan |         nan     |
|        45854 | Afghanistan |         nan     |
|        47110 | Afghanistan |         nan     |
|        49702 | Afghanistan |         nan     |
|        50767 | Afghanistan |         nan     |
|        51859 | Afghanistan |         nan     |
|        58760 | Afghanistan |         nan     |
|        59560 | Afghanistan |         nan     |
|        60569 | Afghanistan |         nan     |
|        60946 | Afghanistan |         nan     |
|        62168 | Afghanistan |         nan     |
|        62525 | Afghanistan |         nan     |
|        74386 | Afghanistan |         nan     |
|        80926 | Afghanistan |         nan     |
|        85715 | Afghanistan |         nan     |
|        85825 | Afghanistan |         nan     |
|        86362 | Afghanistan |         nan     |
|        86932 | Afghanistan |         nan     |
|        87091 | Afghanistan |         nan     |
|        88582 | Afghanistan |         nan     |
|        88731 | Afghanistan |         nan     |
|         2782 | Afghanistan |         nan     |
|        63019 | Afghanistan |         nan     |
|        38308 | Albania     |      187668     |
|         3787 | Albania     |      114550     |
|         9270 | Albania     |       74474     |
|         6716 | Albania     |       60000     |
|        10303 | Albania     |       57300     |
|        66280 | Albania     |       41244     |

*** Sort dataframe based on salary
#+begin_src python 
df['ConvertedComp'].nlargest(10)
#+end_src
#+begin_src 
25983    2000000.0
87896    2000000.0
22013    2000000.0
28243    2000000.0
72732    2000000.0
78151    2000000.0
80200    2000000.0
52132    2000000.0
75561    2000000.0
32250    2000000.0
Name: ConvertedComp, dtype: float64
#+end_src
#+begin_src python 
df.nsmallest(10, 'ConvertedComp')
#+end_src
#+begin_src 
Respondent
722      0.0
28638    0.0
13825    0.0
40913    0.0
30630    0.0
69049    0.0
19340    0.0
16214    0.0
29355    0.0
1685     0.0
Name: ConvertedComp, dtype: float64
#+end_src
* Grouping and Aggregates
file:./pandas-sub-doc/Pandas-Grouping-Aggregates.html
#+begin_src python 
dir_name= '/media/jayradhey/myVolume/WorkSpace_Python/pandas/'
file_name='data/survey_results_public.csv'
schema_name='data/survey_results_schema.csv'
schema_path=dir_name + schema_name
file_path =  dir_name + file_name
print(file_path, schema_path)

df = pd.read_csv(file_path, index_col='Respondent') # , index_col='Respondent'
schema_df = pd.read_csv(schema_path, index_col='Column')
#+end_src

#+begin_src python 
df.describe()
#+end_src

|       |   CompTotal | ConvertedComp | WorkWeekHrs | CodeRevHrs |     Age |
|-------+-------------+---------------+-------------+------------+---------|
| count |       55945 |         55823 |       64503 |      49790 |   79210 |
| mean  | 5.51901e+11 |        127111 |     42.1272 |    5.08431 | 30.3367 |
| std   | 7.33193e+13 |        284152 |     37.2876 |    5.51393 | 9.17839 |
| min   |           0 |             0 |           1 |          0 |       1 |
| 25%   |       20000 |       25777.5 |          40 |          2 |      24 |
| 50%   |       62000 |         57287 |          40 |          4 |      29 |
| 75%   |      120000 |        100000 |       44.75 |          6 |      35 |
| max   |       1e+16 |         2e+06 |        4850 |         99 |      99 |

* Cleaning-Data
Snipped file:./pandas-sub-doc/Pandas-Cleaning-Data.html
Stackoverflow file:./pandas-sub-doc/Pandas-Cleaning-Data-stackoverflow.html
* pandas with datetime
https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime

#+begin_src python
raw_data["date"] =  pd.to_datetime(raw_data["data"], format='%d%b%Y:%H:%M:%S.%f')
# Note: the format argument isn't required. to_datetime is smart. Go ahead and try it without trying to match your data


# If more than one colum 
df[["col1", "col2", "col3"]] = df[["col1", "col2", "col3"]].apply(pd.to_datetime)

# TO remove the timezone from the dataframe

df["date"] =  pd.to_datetime(df["date"])
df.set_index("date",inplace=True)
df.index = pd.to_datetime(df.index)      # convert col='date' to datetime object
df =df.tz_localize(None)                 # remove timezone 

#+end_src

file:./pandas-sub-doc/Datetime-Timeseries.html
* TODO Join
** Inner Join
** Outer Join
** Right Join
** Left Join 
* TODO Horizontal Join
** TODO pandas append

#+begin_src python
def historical_data(ticker,from_date,to_date,interval):
    df = pd.DataFrame()   
    # print(from_date)
    # print(to_date)
    # print(interval_limit[interval])
    interval_limit = {'minute': 60, '3minute':100, '5minute':100,'10minute':100,'15minute':200,'30minute':200,'60minute':400,'day':2000}
    # print(interval_limit['minute'])
    kite_limit = interval_limit[interval]
    while True:
        if from_date >= (to_date - dt.timedelta(kite_limit)):                     #if from_date is within the 60 days limit
            df_temp = fetchOHLC_by_date(ticker,interval,from_date,to_date)
            df = df.append(df_temp)
    #         df.append(pd.DataFrame(kite.historical_data(ticker, from_date, to_date, interval)))
            break
        else:                                                            #if from_date has more than 60 days limit
            to_date_new = from_date + dt.timedelta(kite_limit)
            df_temp = fetchOHLC_by_date(ticker,interval,from_date,to_date)
            df = df.append(df_temp)
            from_date = to_date_new

    return df
display(historical_data(ticker,from_date,to_date,interval))
#+end_src

#+begin_quote
/tmp/ipykernel_53393/823271065.py:12: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. \\
  df.append(df_temp)
#+end_quote
** pandas concat
https://brettromero.com/pandas-append-and-concat/
#+begin_src 
df_new=pd.concat([df_new,df_temp]) # df_new.append(df_temp, sort=True)
#+end_src

#+begin_src python
import yfinance as yf
import pandas as pd
# get ohlcv data for any ticker by start date and end date
data_1 = yf.download("MSFT", start="2020-04-01", end="2020-04-15")
#display(data_1)

data_2 = yf.download("MSFT", start="2020-04-16", end="2020-04-30")
#display(data_2)


data = pd.concat([data_1, data_2])
#display(data)
#+end_src


#+begin_src 

#+end_src
* TODO Copy or shallow copy
by default pandas dataframe '=' will not create new object memory but the new object will use the old object which it is assigned to. \\
Inorder to manupulate the data without effecting  the original dataframe we need to copy the data object in seperate memory using copy method
#+begin_src 
df = historialData['FB'].copy()
#+end_src

* Resampling

https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases


https://stackoverflow.com/questions/61605415/how-can-we-resample-ohlcv-1-minute-pandas-dataframe-into-a-5-minute-dataframe

https://www.geeksforgeeks.org/python-pandas-dataframe-resample/
#+begin_src python
# I just copied your top table here to make the df
df = pd.read_clipboard(sep=r"[ ]{2,}")
df = df.set_index(pd.DatetimeIndex(df['date']))

print(df)

    date    Open    High    Low Close   Volume
date                        
2020-04-27 09:31:00 2020-04-27 09:31:00 10.5300 10.5300 10.4100 10.4458 1408654.0
2020-04-27 09:32:00 2020-04-27 09:32:00 10.4450 10.4450 10.3810 10.4100 469467.0
2020-04-27 09:33:00 2020-04-27 09:33:00 10.4000 10.4100 10.3470 10.3766 305665.0
2020-04-27 09:34:00 2020-04-27 09:34:00 10.3742 10.4000 10.3600 10.3850 127815.0
2020-04-27 09:35:00 2020-04-27 09:35:00 10.3850 10.4000 10.3700 10.3714 125987.0
2020-04-27 09:36:00 2020-04-27 09:36:00 10.3700 10.4100 10.3500 10.3500 248228.0
2020-04-27 09:37:00 2020-04-27 09:37:00 10.3500 10.3850 10.3500 10.3570 130435.0
2020-04-27 09:38:00 2020-04-27 09:38:00 10.3600 10.3600 10.3000 10.3000 250145.0
2020-04-27 09:39:00 2020-04-27 09:39:00 10.3000 10.3299 10.2800 10.2999 277293.0
2020-04-27 09:40:00 2020-04-27 09:40:00 10.2950 10.2950 10.2200 10.2200 333785.0
2020-04-27 09:41:00 2020-04-27 09:41:00 10.2280 10.2300 10.1500 10.1550 292010.0
2020-04-27 09:42:00 2020-04-27 09:42:00 10.1597 10.2100 10.1500 10.1900 314917.0
2020-04-27 09:43:00 2020-04-27 09:43:00 10.1890 10.2180 10.1800 10.2114 293827.0
2020-04-27 09:44:00 2020-04-27 09:44:00 10.2200 10.2500 10.1900 10.1902 317016.0
2020-04-27 09:45:00 2020-04-27 09:45:00 10.1950 10.2100 10.1342 10.1396 296248.0


df_rs = df.resample('5T', label='right', closed='right').agg({'Open':'first',
                                                                 'High':'max',
                                                                 'Low':'min',
                                                                 'Close':'last',
                                                                 'Volume':'sum'})

print(df_rs)

    Open    High    Low Close   Volume
date                    
2020-04-27 09:35:00 10.530  10.53   10.3470 10.3714 2437588.0
2020-04-27 09:40:00 10.370  10.41   10.2200 10.2200 1239886.0
2020-04-27 09:45:00 10.228  10.25   10.1342 10.1396 1514018.0
#+end_src
* Aggreagtes for ohlc
#+begin_src python
df = pd.read_csv('CRUDEOIL_MCX.csv', names=['last_price','date'],index_col=1,parse_dates=True)
df = pd.DataFrame(df)
data=df['last_price'].resample('1min').ohlc()
#+end_src

